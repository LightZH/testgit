{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "\n",
    "AlexNet与LeNet网络架构基本相似，只是AlexNet更深，使用了8层卷积神经网络（5个卷积层，3个最大汇聚层），并且使用的激活函数不再是Sigmoid而是ReLU，其具体[网络架构](../image/AlexNet.jpg)如下:\n",
    "<div align=\"center\">\n",
    "<img src=\"https://s2.loli.net/2022/04/12/woAXRiu9qeG4l3P.jpg\" width=\"30%\">\n",
    "<br>LeNet架构图（左）AlexNet架构图（右）\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于pytorch实现AlexNet网络的模型搭建如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \"\"\"\n",
    "        num_classes:分类的数量\n",
    "        in_channels:原始图像通道数,灰度图像为1,彩色图像为3\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.lenet = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 96, kernel_size=11, stride=4, padding=2),  # (224 - 11 + 4) / 4 + 1 = 55.25   [3, 224, 224] -> [96, 55, 55]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (55 + 2 - 3) / 2 = 27   [96, 55, 55] -> [96, 27, 27]\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),  # (27 - 5 + 4) / 1 + 1 = 27   [96, 27, 27] -> [256, 27, 27]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # (27 - 3 + 2) / 2 = 13   [256, 27, 27] -> [256, 13, 13]\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),  # (13 - 3 + 2) / 1 + 1 = 13   [256, 13, 13] -> [384, 13, 13]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),  # (13 - 3 + 2) / 1 + 1 = 13   [384, 13, 13] -> [384, 13, 13]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # (13 - 3 + 2) / 1 + 1 = 13   [384, 13, 13] -> [256, 13, 13]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # (13 - 3 + 2) / 2 = 6   [256, 13, 13] -> [256, 6, 6]\n",
    "            nn.Flatten(), # 展平\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)) # 不需要使用激活函数，因为softmax激活函数被嵌入在交叉熵函数中\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.lenet(x)\n",
    "        # probas = F.softmax(output, dim=1)\n",
    "        return output #, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一次卷积操作时，不能完全便利整个图像，应该在左、上添两列零，右、下添一列零能完全遍历；使用padding=2填充，在操作过程中会自动省去多余数据，故影响不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型检验\n",
    "\n",
    "打印出各层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: \t torch.Size([1, 96, 55, 55])\n",
      "ReLU output shape: \t torch.Size([1, 96, 55, 55])\n",
      "MaxPool2d output shape: \t torch.Size([1, 96, 27, 27])\n",
      "Conv2d output shape: \t torch.Size([1, 256, 27, 27])\n",
      "ReLU output shape: \t torch.Size([1, 256, 27, 27])\n",
      "MaxPool2d output shape: \t torch.Size([1, 256, 13, 13])\n",
      "Conv2d output shape: \t torch.Size([1, 384, 13, 13])\n",
      "ReLU output shape: \t torch.Size([1, 384, 13, 13])\n",
      "Conv2d output shape: \t torch.Size([1, 384, 13, 13])\n",
      "ReLU output shape: \t torch.Size([1, 384, 13, 13])\n",
      "Conv2d output shape: \t torch.Size([1, 256, 13, 13])\n",
      "ReLU output shape: \t torch.Size([1, 256, 13, 13])\n",
      "MaxPool2d output shape: \t torch.Size([1, 256, 6, 6])\n",
      "Flatten output shape: \t torch.Size([1, 9216])\n",
      "Linear output shape: \t torch.Size([1, 4096])\n",
      "ReLU output shape: \t torch.Size([1, 4096])\n",
      "Dropout output shape: \t torch.Size([1, 4096])\n",
      "Linear output shape: \t torch.Size([1, 4096])\n",
      "ReLU output shape: \t torch.Size([1, 4096])\n",
      "Dropout output shape: \t torch.Size([1, 4096])\n",
      "Linear output shape: \t torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "num_classes = 1000\n",
    "in_channels = 3\n",
    "X = torch.rand(size=(1, in_channels, 224, 224), dtype=torch.float32)\n",
    "net = AlexNet(in_channels, num_classes)\n",
    "for layer in net.lenet:\n",
    "  X = layer(X)\n",
    "  print(layer.__class__.__name__,'output shape: \\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "*后补*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型检测\n",
    "\n",
    "*后补*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet与LeNet不同\n",
    "\n",
    "* 网络更深，使用8层网络进行训练\n",
    "* 激活函数改进，使用ReLU替代Sigmoid\n",
    "* 池化层采用Maxpool2d代替Avgpool2d\n",
    "* 在全连接层处使用Dropout丢弃法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
